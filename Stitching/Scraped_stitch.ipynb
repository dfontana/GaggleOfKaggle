{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and merge CSVs\n",
    "ticker = pd.read_csv('../data/asx-tickers.csv')\n",
    "combined_headlines = Path('../data/scraped/combined_headlines.csv')\n",
    "if not combined_headlines.is_file():\n",
    "    head_1 = pd.read_csv('../data/scraped/2006_2010.csv')\n",
    "    head_2 = pd.read_csv('../data/scraped/2010_2010.csv')\n",
    "    head_3 = pd.read_csv('../data/scraped/2010_2016.csv')\n",
    "    head_4 = pd.read_csv('../data/scraped/2016_2017.csv')\n",
    "    headlines = head_1.append([head_2, head_3, head_4])\n",
    "    headlines.to_csv('../data/scraped/combined_headlines.csv')\n",
    "\n",
    "headlines = pd.read_csv('../data/scraped/combined_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the headlines (lightly) - drop excess characters and duplicates\n",
    "stripped = headlines.titles.apply(lambda x: x.strip('\"\\' ' ))\n",
    "headlines_clean = headlines[~headlines.titles.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records that don't have any company words\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "\n",
    "def normalize(s):\n",
    "    return s.lower().translate(translator).split()\n",
    "\n",
    "def incompwords(s):\n",
    "    headwords = set(normalize(s))\n",
    "    return len(headwords & compwords) > 0\n",
    "\n",
    "compwords = set(normalize((\" \".join(ticker['company'].values))))\n",
    "headlines_filtered = headlines_clean[headlines_clean.titles.apply(incompwords)]\n",
    "\n",
    "print(\"Headlines with company names: \",headlines_filtered.shape[0])\n",
    "headlines_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the Named Entities in each headline, associating them to their own\n",
    "# column within the dataframe. Save this as CSV to prevent later processing needs.\n",
    "# https://stackoverflow.com/questions/36255291/extract-city-names-from-text-using-python\n",
    "\n",
    "# !!! This will only run if headline_entity_id.csv isn't in the data folder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "def identify_entities():\n",
    "    f = headlines_filtered.titles.tolist()\n",
    "    matches = []\n",
    "    for line in tqdm(f, total=len(f), unit=\"headlines\"):\n",
    "        sentences = nltk.sent_tokenize(line)\n",
    "        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "        chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "        entities = []\n",
    "        for tree in chunked_sentences:\n",
    "            entities.extend(extract_entity_names(tree))\n",
    "            \n",
    "        if len(entities) == 0:\n",
    "            matches.append(np.nan)\n",
    "        else:\n",
    "            matches.append(\", \".join(entities))\n",
    "\n",
    "    series = pd.Series(matches)\n",
    "    out = headlines_filtered.copy()\n",
    "    out['entities'] = series.values\n",
    "    out = out.dropna()\n",
    "    out.to_csv(\"../data/headline_entity_id.csv\")\n",
    "    \n",
    "entitymatch = Path('../data/headline_entity_id.csv')\n",
    "if not entitymatch.is_file():\n",
    "    # Build the entity matches\n",
    "    identify_entities()\n",
    "else:\n",
    "    print(\"Entities already generated. Delete CSV to rebuild (if desired).\")  \n",
    "headline_entities = pd.read_csv(\"../data/headline_entity_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization Code: Will split data evenly amoungst threads\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "cores = cpu_count()\n",
    " \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, cores)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to fuzzy match\n",
    "tqdm().pandas(desc=\"Fuzzy Match\")\n",
    "FIRSTN = 15000\n",
    "\n",
    "def fuzzy_match(x, choices, scorer, cutoff):\n",
    "    if x == '' or type(x) != str:\n",
    "        return\n",
    "    return process.extractOne(x, choices=choices, scorer=scorer, score_cutoff=cutoff)\n",
    "    \n",
    "\n",
    "# Parallelization function, to run on each split of data\n",
    "def appfun(df):\n",
    "    return df.loc[:FIRSTN, 'entities'].progress_apply(\n",
    "        fuzzy_match,\n",
    "        args=(\n",
    "            ticker.loc[:, 'company'].map(lambda x: x.lower()), \n",
    "            fuzz.ratio,\n",
    "            90\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run in parallel (pick one)\n",
    "# matching_results = parallelize(headline_entities, appfun)\n",
    "\n",
    "# Run in thread (pick one)\n",
    "# matching_results = appfun(headline_entities)\n",
    "\n",
    "# Compare\n",
    "# test = headline_entities[:FIRSTN].copy()\n",
    "# test['match'] = matching_results\n",
    "# test[test['match'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt the Position Based Algorithm\n",
    "tick_df = ticker.copy();\n",
    "tick_df['set'] = tick_df['company'].apply(lambda x: set(x.title().split(\" \")))\n",
    "tick_df['list'] = tick_df['company'].apply(lambda x: x.title().split(\" \"))\n",
    "\n",
    "MIN_SCORE = 0.3\n",
    "def findMatches(headline):\n",
    "    matchedCompanies = []\n",
    "    \n",
    "    # Return if the headline has no entities.\n",
    "    if headline['entities'] == '' or type(headline['entities']) != str:\n",
    "        return\n",
    "    \n",
    "    entity_set = set(headline['entities'].split(', '))\n",
    "    \n",
    "    # Iterate over sets of companies\n",
    "    for _,company in tick_df.iterrows():\n",
    "        matchingHeads = []\n",
    "        companyMatchScore = 0\n",
    "        company_set = company['set']\n",
    "        \n",
    "        shared = entity_set & company_set\n",
    "        if len(shared) > 0:\n",
    "            for word in shared:\n",
    "                idx = company['list'].index(word)\n",
    "                companyMatchScore += (1/(idx+1))\n",
    "        normalizedScore = companyMatchScore / len(company_set)\n",
    "        if normalizedScore >= MIN_SCORE:\n",
    "            matchedCompanies.append((company['company'], normalizedScore))\n",
    "            \n",
    "                \n",
    "    matchedCompanies.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matchedCompanies[:10]\n",
    "\n",
    "# Run in parallel\n",
    "tqdm.pandas(desc=\"Position Match\")\n",
    "def appfun(df):\n",
    "    return df.progress_apply(findMatches, axis=1)\n",
    "# matching_results = parallelize(headline_entities, appfun)\n",
    "matching_results = appfun(headline_entities)\n",
    "# Compare\n",
    "test = headline_entities.copy()\n",
    "test['match'] = matching_results\n",
    "test['match'] = test['match'].dropna()\n",
    "test.to_csv(\"../data/position_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
