{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and merge CSVs\n",
    "ticker = pd.read_csv('../data/asx-tickers.csv')\n",
    "head_1 = pd.read_csv('../data/scraped/2006_2010.csv')\n",
    "head_2 = pd.read_csv('../data/scraped/2010_2010.csv')\n",
    "head_3 = pd.read_csv('../data/scraped/2010_2016.csv')\n",
    "head_4 = pd.read_csv('../data/scraped/2016_2017.csv')\n",
    "headlines = head_1.append([head_2, head_3, head_4])\n",
    "combined_headlines = Path('../data/scraped/combined_headlines.csv')\n",
    "if not combined_headlines.is_file():\n",
    "    headlines.to_csv('../data/scraped/combined_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the headlines (lightly) - drop excess characters and duplicates\n",
    "stripped = headlines.titles.apply(lambda x: x.strip('\"\\' ' ))\n",
    "headlines_clean = headlines[~headlines.titles.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines with company names:  592608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Russia completes Ukraine gas cut-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>High winds cause havoc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Russia takes over G8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                  titles\n",
       "0  2006-01-01   Russia completes Ukraine gas cut-off \n",
       "1  2006-01-01                 High winds cause havoc \n",
       "3  2006-01-01                   Russia takes over G8 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop records that don't have any company words\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "\n",
    "def normalize(s):\n",
    "    return s.lower().translate(translator).split()\n",
    "\n",
    "def incompwords(s):\n",
    "    headwords = set(normalize(s))\n",
    "    return len(headwords & compwords) > 0\n",
    "\n",
    "compwords = set(normalize((\" \".join(ticker['company'].values))))\n",
    "headlines_filtered = headlines_clean[headlines_clean.titles.apply(incompwords)]\n",
    "\n",
    "print(\"Headlines with company names: \",headlines_filtered.shape[0])\n",
    "headlines_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/koss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/koss/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/koss/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Identify the Named Entities in each headline, associating them to their own\n",
    "# column within the dataframe. Save this as CSV to prevent later processing needs.\n",
    "# https://stackoverflow.com/questions/36255291/extract-city-names-from-text-using-python\n",
    "\n",
    "# !!! This will only run if headline_entity_id.csv isn't in the data folder\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "def identify_entities():\n",
    "    f = headlines_filtered.titles.tolist()\n",
    "    matches = []\n",
    "    for line in tqdm(f, total=len(f), unit=\"headlines\"):\n",
    "        sentences = nltk.sent_tokenize(line)\n",
    "        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "        chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "        entities = []\n",
    "        for tree in chunked_sentences:\n",
    "            entities.extend(extract_entity_names(tree))\n",
    "\n",
    "        matches.append(\", \".join(entities))\n",
    "\n",
    "    series = pd.Series(matches)\n",
    "    out = headlines_filtered.copy()\n",
    "    out['entities'] = series.values\n",
    "    out.to_csv(\"../data/headline_entity_id.csv\")\n",
    "    \n",
    "entitymatch = Path('../data/headline_entity_id.csv')\n",
    "if not entitymatch.is_file():\n",
    "    # Build the entity matches\n",
    "    identify_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
