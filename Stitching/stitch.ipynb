{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSVs needed to make the join.\n",
    "ticker = pd.read_csv('data/asx-tickers.csv')\n",
    "ticker_stripped = pd.read_csv('data/asx-tickers_stripped.csv')\n",
    "headlines = pd.read_csv('data/abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1AD</td>\n",
       "      <td>ADALTA LIMITED</td>\n",
       "      <td>Pharmaceuticals, Biotechnology &amp; Life Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1AG</td>\n",
       "      <td>ALTERRA LIMITED</td>\n",
       "      <td>Commercial &amp; Professional Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker          company                                        industry\n",
       "0    1AD   ADALTA LIMITED  Pharmaceuticals, Biotechnology & Life Sciences\n",
       "1    1AG  ALTERRA LIMITED              Commercial & Professional Services"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out records that don't have any company word mentions\n",
    "import string\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "\n",
    "def normalize(s):\n",
    "    return s.lower().translate(translator).split()\n",
    "\n",
    "def incompwords(s):\n",
    "    headwords = set(normalize(s))\n",
    "    return len(headwords & compwords) > 0\n",
    "\n",
    "compwords = set(normalize((\" \".join(ticker['company'].values))))\n",
    "headlines_filtered = headlines[headlines['headline_text'].apply(incompwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization Code: Will split data evenly amoungst threads\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "cores = cpu_count()\n",
    " \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, cores)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to Fuzzy Match up to FIRSTN records\n",
    "tqdm.pandas(desc=\"Raw Fuzzy Match\")\n",
    "\n",
    "FIRSTN = 1000\n",
    "\n",
    "# Fuzzy match function\n",
    "def fuzzy_match(x, choices, scorer, cutoff):\n",
    "    return process.extractOne(x, choices=choices, scorer=scorer, score_cutoff=cutoff)\n",
    "    \n",
    "\n",
    "# Parallelization function, to run on each split of data\n",
    "def appfun(df):\n",
    "    return df.loc[:FIRSTN, 'headline_text'].progress_apply(\n",
    "        fuzzy_match,\n",
    "        args=(\n",
    "            ticker.loc[:, 'company'], \n",
    "            fuzz.partial_ratio,\n",
    "            80\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run in parallel\n",
    "# matching_results = parallelize(headlines, appfun)\n",
    "# test = headlines[:FIRSTN].copy()\n",
    "# test['match'] = matching_results\n",
    "# test[test['match'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nouns = ['NNS', 'NNP', 'NNPS']\n",
    "string = \"qantas urged to update security in shadow of\".split()\n",
    "tags = nltk.tag.pos_tag(string)\n",
    "filtered = \" \".join([x[0] for x in tags if x[1] in nouns])\n",
    "actual = 'QANTAS AIRWAYS LIMITED'.lower()\n",
    "print(filtered, actual)\n",
    "fuzz.ratio(filtered, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt to Fuzzy Match only the nouns of a sentence, up to FIRSTN records.\n",
    "from fuzzywuzzy import process, fuzz\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tqdm.pandas(desc=\"Fuzzy with only Nouns\")\n",
    "\n",
    "nouns = ['NNS', 'NNP', 'NNPS']\n",
    "\n",
    "FIRSTN = 1000\n",
    "\n",
    "# Fuzzy match function\n",
    "def fuzzy_match(x, choices, scorer, cutoff):\n",
    "    tags = nltk.tag.pos_tag(x.split())\n",
    "    filtered = \" \".join([x[0] for x in tags if x[1] in nouns])\n",
    "    if filtered == '':\n",
    "        return\n",
    "    return process.extractOne(filtered, choices=choices, scorer=scorer, score_cutoff=cutoff)\n",
    "    \n",
    "\n",
    "# Parallelization function, to run on each split of data\n",
    "def appfun(df):\n",
    "    return df.loc[:FIRSTN, 'headline_text'].progress_apply(\n",
    "        fuzzy_match,\n",
    "        args=(\n",
    "            ticker.loc[:, 'company'].map(lambda x: x.lower()), \n",
    "            fuzz.partial_ratio,\n",
    "            65\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run in parallel\n",
    "# matching_results = parallelize(headlines, appfun)\n",
    "# test = headlines[:FIRSTN].copy()\n",
    "# test['match'] = matching_results\n",
    "# test[test['match'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Approach\n",
    "tick_df = ticker.copy();\n",
    "head_df = headlines_filtered.copy().head(1000);\n",
    "head_df['matches'] = None\n",
    "head_df.astype({'matches':'object'})\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "# 1. Vectorize companies and headlines in their stripped form.\n",
    "def vectorize(s):\n",
    "    tokens = normalize(s)\n",
    "    return [ np.array(list(word)) for word in tokens ]\n",
    "\n",
    "def vectorizeFirstHalf(s):\n",
    "    tokens = normalize(s)\n",
    "    return [ np.array(list(word)) for word in tokens[0:len(tokens)//2] ]\n",
    "\n",
    "tick_df['comp_vec'] = tick_df['company'].apply(vectorize)\n",
    "head_df['head_vec'] = head_df['headline_text'].apply(vectorizeFirstHalf)\n",
    "\n",
    "print(head_df['head_vec'])\n",
    "\n",
    "def percentage_matching(W, C):\n",
    "    if len(W) < len(C):\n",
    "        return np.sum(W == C[0:len(W)]) / len(C)\n",
    "    else:\n",
    "        return np.sum(W[0:len(C)] == C) / len(W)\n",
    "\n",
    "# 2. Headline opertion\n",
    "MIN_SCORE = 0.3\n",
    "def findMatches(head_record):\n",
    "    matchedCompanies = []\n",
    "    headWordList = head_record['head_vec']\n",
    "    \n",
    "    for _,comp_record in tick_df.iterrows():\n",
    "        \n",
    "        companyWordList = comp_record['comp_vec']\n",
    "        matchingHeads = []\n",
    "        \n",
    "        \"\"\"\n",
    "        New option: \n",
    "            1. Convert headline and company name to set.\n",
    "            2. Set intersect. \n",
    "            3. For all common words find it's index in the company name\n",
    "            4. Can compute the weighted worth of the match.\n",
    "        \"\"\"\n",
    "        \n",
    "        for wIdx in range(len(headWordList)):\n",
    "            if companyWordList[0][0] != headWordList[wIdx][0]:\n",
    "                continue\n",
    "                          \n",
    "            companyMatchScore = 0\n",
    "            for cIdx,C in enumerate(companyWordList):\n",
    "                if  wIdx+cIdx < len(headWordList):\n",
    "                    weight = 1/(cIdx+1)\n",
    "                    W = headWordList[wIdx+cIdx]\n",
    "                    matches = percentage_matching(W, C)\n",
    "                    if matches == 1.0:\n",
    "                        companyMatchScore += (matches * weight)\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            normalizedScore = companyMatchScore / len(companyWordList) # Normalizes score out of 100%.\n",
    "            if normalizedScore >= MIN_SCORE:\n",
    "                matchedCompanies.append((comp_record['company'], normalizedScore))\n",
    "                \n",
    "    matchedCompanies.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matchedCompanies[:10]\n",
    "\n",
    "# Uncomment for parallel and comment the bottom part\n",
    "tqdm.pandas(desc=\"HALP\")\n",
    "def appfun(df):\n",
    "    print('GO')\n",
    "    return df.progress_apply(findMatches, axis=1)\n",
    "\n",
    "print(\"Looping...\")\n",
    "parallelize(head_df, appfun)\n",
    "\n",
    "# tqdm_notebook().pandas(\"matching\")\n",
    "# head_df['matches'] = head_df.progress_apply(findMatches, axis=1)\n",
    "# head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          act fire witnesses must be aware of defamation\n",
       "2          a g calls for infrastructure protection summit\n",
       "3                air nz staff in aust strike for pay rise\n",
       "4           air nz strike to affect australian travellers\n",
       "5                       ambitious olsson wins triple jump\n",
       "7       aussie qualifier stosur wastes four memphis match\n",
       "8            aust addresses un security council over iraq\n",
       "9              australia is locked into war timetable opp\n",
       "10      australia to contribute 10 million in aid to iraq\n",
       "13          big hopes for launceston cycling championship\n",
       "14                 big plan to boost paroo water supplies\n",
       "15                 blizzard buries united states in bills\n",
       "20        businesses should prepare for terrorist attacks\n",
       "22                call for ethanol blend fuel to go ahead\n",
       "24                           cemeteries miss out on funds\n",
       "25      code of conduct toughens organ donation regula...\n",
       "26           commonwealth bank cuts fixed home loan rates\n",
       "29        councillor to contest wollongong as independent\n",
       "30           council moves to protect tas heritage garden\n",
       "32                council welcomes insurance breakthrough\n",
       "34                     dargo fire threat expected to rise\n",
       "35       death toll continues to climb in s korean subway\n",
       "36               dems hold plebiscite over iraqi conflict\n",
       "38                   de villiers to learn fate on march 5\n",
       "39              digital tv will become commonplace summit\n",
       "40          direct anger at govt not soldiers crean urges\n",
       "41      dispute over at smithton vegetable processing ...\n",
       "43         dying korean subway passengers phoned for help\n",
       "44                   england change three for wales match\n",
       "45      epa still trying to recover chemical clean up ...\n",
       "                              ...                        \n",
       "1446                              us to saddam time is up\n",
       "1447        us vice president to see rail work first hand\n",
       "1449         vic libs face parliament after poll disaster\n",
       "1451     wa opposition appoints new agriculture spokesman\n",
       "1453                                warne counts the cost\n",
       "1455    western countries remain divided over war in iraq\n",
       "1458              woman charged over possessing rare drug\n",
       "1459                  youhana lifts pakistan to 9 for 253\n",
       "1460            abs defends nt population shrinking claim\n",
       "1461     acb report slams warnes vague and unsatisfactory\n",
       "1466             akram claims 500 as pakistan downs dutch\n",
       "1467              alston seizes on glowing telstra report\n",
       "1469                    amp concedes it was overambitious\n",
       "1470                               amp confirms 896m loss\n",
       "1471                       amp losing investor confidence\n",
       "1472    aquaculture council speaks out over kingfish c...\n",
       "1473                 army too small to meet demand on sas\n",
       "1475    asic charges wine giant with bottling up forec...\n",
       "1476           asic urges caution over insurance policies\n",
       "1477    asx achieves 11pc profit increase over six months\n",
       "1478                   asx told mine information withheld\n",
       "1480    atsic deputy ruddock clash over clarkes court ...\n",
       "1481                          aussie dollar hits new high\n",
       "1482                aus sports awards finalists announced\n",
       "1483                  aust airlines looks to asian market\n",
       "1484    australia marks 30 years of diplomacy with vie...\n",
       "1485        australia targeting england win against india\n",
       "1486        beattie hits out at obscene corporate payouts\n",
       "1488         bhp billiton native title efforts recognised\n",
       "1489    black box installation not held up scully insists\n",
       "Name: headline_text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_filtered['headline_text'].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Union Approach for faster computation.\n",
    "tick_df = ticker.copy();\n",
    "head_df = headlines_filtered.copy();\n",
    "head_df['matches'] = None\n",
    "head_df.astype({'matches':'object'})\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "# 1. Vectorize companies and headlines in their stripped form.\n",
    "tick_df['comp_set'] = tick_df['company'].apply(lambda x: set(normalize(x)))\n",
    "head_df['head_set'] = head_df['headline_text'].apply(lambda x: set(normalize(x)))\n",
    "tick_df['comp_list'] = tick_df['company'].apply(lambda x: normalize(x))\n",
    "head_df['head_list'] = head_df['headline_text'].apply(lambda x: normalize(x))\n",
    "\n",
    "# 2. Headline opertion\n",
    "MIN_SCORE = 0.3\n",
    "def findMatches(head_record):\n",
    "    matchedCompanies = []\n",
    "    headWordList = head_record['head_set']\n",
    "    \n",
    "    for _,comp_record in tick_df.iterrows():\n",
    "        \n",
    "        companyWordList = comp_record['comp_set']\n",
    "        matchingHeads = []\n",
    "        \n",
    "        \"\"\"\n",
    "        New option: \n",
    "            1. Convert headline and company name to set.\n",
    "            2. Set intersect. \n",
    "            3. For all common words find it's index in the company name\n",
    "            4. Can compute the weighted worth of the match.\n",
    "        \"\"\"\n",
    "        \n",
    "        companyMatchScore = 0\n",
    "        shared = headWordList & companyWordList\n",
    "        if len(shared) > 0:\n",
    "            for word in shared:\n",
    "                idx = comp_record['comp_list'].index(word)\n",
    "                companyMatchScore += (1/(idx+1))\n",
    "        normalizedScore = companyMatchScore / len(companyWordList)\n",
    "        if normalizedScore >= MIN_SCORE:\n",
    "            matchedCompanies.append((comp_record['company'], normalizedScore))\n",
    "            \n",
    "                \n",
    "    matchedCompanies.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matchedCompanies[:10]\n",
    "\n",
    "# Uncomment for parallel and comment the bottom part\n",
    "tqdm.pandas(desc=\"HALP\")\n",
    "def appfun(df):\n",
    "    print('GO')\n",
    "    return df.progress_apply(findMatches, axis=1)\n",
    "\n",
    "print(\"Looping...\")\n",
    "parallelize(head_df, appfun)\n",
    "\n",
    "# tqdm_notebook().pandas(\"matching\")\n",
    "# head_df['matches'] = head_df.progress_apply(findMatches, axis=1)\n",
    "# head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Looping...\n",
      "GO\n",
      "GO\n",
      "GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "HALP:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HALP: 100%|██████████| 250/250 [01:20<00:00,  2.51it/s]\n",
      "\n",
      "HALP: 100%|█████████▉| 249/250 [01:20<00:00,  2.58it/s]\n",
      "HALP: 100%|██████████| 250/250 [01:21<00:00,  2.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1                  []\n",
       "2                  []\n",
       "3                  []\n",
       "4                  []\n",
       "5                  []\n",
       "7                  []\n",
       "8                  []\n",
       "9                  []\n",
       "10                 []\n",
       "13                 []\n",
       "14                 []\n",
       "15                 []\n",
       "20                 []\n",
       "22                 []\n",
       "24                 []\n",
       "25                 []\n",
       "26                 []\n",
       "29                 []\n",
       "30                 []\n",
       "32                 []\n",
       "34                 []\n",
       "35                 []\n",
       "36                 []\n",
       "38                 []\n",
       "39                 []\n",
       "40                 []\n",
       "41                 []\n",
       "43                 []\n",
       "44                 []\n",
       "45                 []\n",
       "            ...      \n",
       "1446               []\n",
       "1447               []\n",
       "1449               []\n",
       "1451               []\n",
       "1453               []\n",
       "1455               []\n",
       "1458               []\n",
       "1459               []\n",
       "1460               []\n",
       "1461               []\n",
       "1466               []\n",
       "1467               []\n",
       "1469    [(AMP , 1.0)]\n",
       "1470    [(AMP , 1.0)]\n",
       "1471    [(AMP , 1.0)]\n",
       "1472               []\n",
       "1473               []\n",
       "1475               []\n",
       "1476               []\n",
       "1477    [(ASX , 1.0)]\n",
       "1478    [(ASX , 1.0)]\n",
       "1480               []\n",
       "1481               []\n",
       "1482               []\n",
       "1483               []\n",
       "1484               []\n",
       "1485               []\n",
       "1486               []\n",
       "1488               []\n",
       "1489               []\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set Union Approach for faster computation.\n",
    "tick_df = ticker_stripped.copy();\n",
    "head_df = headlines_filtered.copy().head(1000);\n",
    "head_df['matches'] = None\n",
    "head_df.astype({'matches':'object'})\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "# 1. Vectorize companies and headlines in their stripped form.\n",
    "tick_df['comp_set'] = tick_df['company'].apply(lambda x: set(normalize(x)))\n",
    "head_df['head_set'] = head_df['headline_text'].apply(lambda x: set(normalize(x)))\n",
    "tick_df['comp_list'] = tick_df['company'].apply(lambda x: normalize(x))\n",
    "head_df['head_list'] = head_df['headline_text'].apply(lambda x: normalize(x))\n",
    "\n",
    "# 2. Headline opertion\n",
    "MIN_SCORE = 0.8\n",
    "def findMatches(head_record):\n",
    "    matchedCompanies = []\n",
    "    headWordList = head_record['head_set']\n",
    "    \n",
    "    for _,comp_record in tick_df.iterrows():\n",
    "        \n",
    "        companyWordList = comp_record['comp_set']\n",
    "        matchingHeads = []\n",
    "        \n",
    "        \"\"\"\n",
    "        New option: \n",
    "            1. Convert headline and company name to set.\n",
    "            2. Set intersect. \n",
    "            3. For all common words find it's index in the company name\n",
    "            4. Can compute the weighted worth of the match.\n",
    "        \"\"\"\n",
    "        \n",
    "        companyMatchScore = 0\n",
    "        shared = headWordList & companyWordList\n",
    "        if len(shared) > 0:\n",
    "            for word in shared:\n",
    "                idx = comp_record['comp_list'].index(word)\n",
    "                companyMatchScore += (1/(idx+1))\n",
    "        normalizedScore = companyMatchScore / len(companyWordList)\n",
    "        if normalizedScore >= MIN_SCORE:\n",
    "            matchedCompanies.append((comp_record['company'], normalizedScore))\n",
    "            \n",
    "                \n",
    "    matchedCompanies.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matchedCompanies[:10]\n",
    "\n",
    "# Uncomment for parallel and comment the bottom part\n",
    "tqdm.pandas(desc=\"HALP\")\n",
    "def appfun(df):\n",
    "    print('GO')\n",
    "    return df.progress_apply(findMatches, axis=1)\n",
    "\n",
    "print(\"Looping...\")\n",
    "parallelize(head_df, appfun)\n",
    "\n",
    "# tqdm_notebook().pandas(\"matching\")\n",
    "# head_df['matches'] = head_df.progress_apply(findMatches, axis=1)\n",
    "# head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Approach\n",
    "\n",
    "\"\"\"\n",
    "First we want to clean up the company column to make computing percentage of matching \n",
    "characters easier:\n",
    "\n",
    "1. lowercase the entire company column\n",
    "   a. remove punctuation (except spaces).\n",
    "   b. Tokenize on spaces and produce a list of each word vectorized.\n",
    "   \n",
    "Then we want to look at each headline, first cleaning it so it can match the same as\n",
    "the companies would (no punctuation).\n",
    "   \n",
    "2. for each headline:\n",
    "    a. remove punctuation (except spaces).\n",
    "    b. tokenize on spaces and produce a list of each work vectorized.\n",
    "    \n",
    "    \n",
    "    With each cleaned headline, we now have a list of words inside it. For each company we\n",
    "    also have a list of cleaned words. So now we want to try to score each company on how\n",
    "    well its words matches the headline words.\n",
    "    \n",
    "    c. for each company:\n",
    "        i. matchedCompanies = []\n",
    "        \n",
    "        \n",
    "        We do this by under the notion the first word of a company MUST be present. So we\n",
    "        find each word in the headline that starts with the company's first letter. From there\n",
    "        we compare each word starting from this word in the headline to each word in the\n",
    "        company. We do a percentage matching.\n",
    "        \n",
    "        We also want to weigh each successive word less and less, and we want to have a scoring\n",
    "        system where higher is better. This allows headlines like \"quantas under fire for...\"\n",
    "        to score well with companies like \"quantas airlines limited\". If Quantas is found, it should\n",
    "        score super high, yet not allow the mismatch between \"under\" and \"airlines\" to drag it down.\n",
    "        In the event two companies start with the same first word, then the second word matching would\n",
    "        help distinguish.\n",
    "        \n",
    "        \n",
    "        ii. For each word (W) in headline:\n",
    "            1. If it doesn't start with the company's first word first letter, skip\n",
    "            2. CompanyMatchScore = 0\n",
    "            2. for i,C in Company Word List: (where C is the ith word in their name list)\n",
    "            3.   if W+i exists\n",
    "            4.     CompanyMatchScore += percentage_matching(W+i, C) * Weight (the first word should weight far more than latter words)\n",
    "            \n",
    "            \n",
    "            We want to normalize this CompanyMatchScore to be out of 100% when done, as we'll later need\n",
    "            to compare how different company names of different lengths performed. If we don't normalize\n",
    "            then names with more words will score higher than those with less.\n",
    "            \n",
    "            \n",
    "            5. normalizedScore = companyMatchScore / len(CompanyWordList) # Normalizes score out of 100%.\n",
    "            6. matchCompanies.append((company, normalizedScore))\n",
    "            \n",
    "        \n",
    "        Before moving tot the next company, we'll sort the matches to make later filtering easier. We'll\n",
    "        then append these matches into a new column associated with the headlines ('matches'). We can limit\n",
    "        the number of matches to keep to some top N (10? 5?) to prevent filling memory with huge vectors.\n",
    "        \n",
    "        \n",
    "        iii. Sort matchedCompanies by their normalizedScore\n",
    "        iV. df.loc[headlineIndex, 'matches'] = matchedCompanies[:10].\n",
    "\n",
    "\n",
    "Now we have all our matches with their headlines, we can inspect how well it did and start keeping or discarding matches.\n",
    "The thought is that some headlines won't have any companies associated so their match score should be low. We'll drop\n",
    "matches that don't meet a threshold and retain the highest one above the threshold (for those that do meet the threshold).\n",
    "This will leave some false matches behind (for example if \"Bell Labs\" is a company and a headline has \"time to ring the bell\"\n",
    "in it). But once we match enough companies, we can probably keep only those companies who had more than N headlines matched \n",
    "to them for further analysis, allowing us to account for these false positives:\n",
    "        \n",
    "\n",
    "3. Ideally, the correct answer is the first element in each match. \n",
    "    a. If the first match is > some percentage threshold, keep it as the match\n",
    "    b. else the headline had no matches and use NA. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
