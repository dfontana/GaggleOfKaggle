{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSVs needed to make the join.\n",
    "ticker = pd.read_csv('data/asx-tickers.csv')\n",
    "headlines = pd.read_csv('data/abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1AD</td>\n",
       "      <td>ADALTA LIMITED</td>\n",
       "      <td>Pharmaceuticals, Biotechnology &amp; Life Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1AG</td>\n",
       "      <td>ALTERRA LIMITED</td>\n",
       "      <td>Commercial &amp; Professional Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker          company                                        industry\n",
       "0    1AD   ADALTA LIMITED  Pharmaceuticals, Biotechnology & Life Sciences\n",
       "1    1AG  ALTERRA LIMITED              Commercial & Professional Services"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DF into chunks, and process each in a seperate thread on your CPU\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "cores = cpu_count()\n",
    " \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, cores)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to Fuzzy Match up to FIRSTN records\n",
    "\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"HALP\")\n",
    "\n",
    "FIRSTN = 1000\n",
    "\n",
    "# Fuzzy match function\n",
    "def fuzzy_match(x, choices, scorer, cutoff):\n",
    "    return process.extractOne(x, choices=choices, scorer=scorer, score_cutoff=cutoff)\n",
    "    \n",
    "\n",
    "# Parallelization function, to run on each split of data\n",
    "def appfun(df):\n",
    "    return df.loc[:FIRSTN, 'headline_text'].progress_apply(\n",
    "        fuzzy_match,\n",
    "        args=(\n",
    "            ticker.loc[:, 'company'], \n",
    "            fuzz.partial_ratio,\n",
    "            80\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run in parallel\n",
    "# matching_results = parallelize(headlines, appfun)\n",
    "# test = headlines[:FIRSTN].copy()\n",
    "# test['match'] = matching_results\n",
    "# test[test['match'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/koss/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "qantas qantas airways limited\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sandbox\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nouns = ['NNS', 'NNP', 'NNPS']\n",
    "string = \"qantas urged to update security in shadow of\".split()\n",
    "tags = nltk.tag.pos_tag(string)\n",
    "filtered = \" \".join([x[0] for x in tags if x[1] in nouns])\n",
    "actual = 'QANTAS AIRWAYS LIMITED'.lower()\n",
    "print(filtered, actual)\n",
    "fuzz.ratio(filtered, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/koss/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## Attempt to Fuzzy Match only the nouns of a sentence, up to FIRSTN records.\n",
    "\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tqdm.pandas(desc=\"HALP\")\n",
    "\n",
    "nouns = ['NNS', 'NNP', 'NNPS']\n",
    "\n",
    "FIRSTN = 1000\n",
    "\n",
    "# Fuzzy match function\n",
    "def fuzzy_match(x, choices, scorer, cutoff):\n",
    "    tags = nltk.tag.pos_tag(x.split())\n",
    "    filtered = \" \".join([x[0] for x in tags if x[1] in nouns])\n",
    "    if filtered == '':\n",
    "        return\n",
    "    return process.extractOne(filtered, choices=choices, scorer=scorer, score_cutoff=cutoff)\n",
    "    \n",
    "\n",
    "# Parallelization function, to run on each split of data\n",
    "def appfun(df):\n",
    "    return df.loc[:FIRSTN, 'headline_text'].progress_apply(\n",
    "        fuzzy_match,\n",
    "        args=(\n",
    "            ticker.loc[:, 'company'].map(lambda x: x.lower()), \n",
    "            fuzz.partial_ratio,\n",
    "            65\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run in parallel\n",
    "# matching_results = parallelize(headlines, appfun)\n",
    "# test = headlines[:FIRSTN].copy()\n",
    "# test['match'] = matching_results\n",
    "# test[test['match'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Looping...\n",
      "GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HALP:   0%|          | 228/137959 [00:33<5:19:27,  7.19it/s]Process ForkPoolWorker-50:\n",
      "Process ForkPoolWorker-55:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-56:\n",
      "Process ForkPoolWorker-53:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-be59fa339bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Looping...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# tqdm_notebook().pandas(\"matching\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-1d737755f19e>\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Process ForkPoolWorker-51:\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-125-be59fa339bb0>\", line 61, in appfun\n",
      "    return df.progress_apply(findMatches, axis=1)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 572, in inner\n",
      "    result = getattr(df, df_function)(wrapper, *args, **kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\", line 4877, in apply\n",
      "    ignore_failures=ignore_failures)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\", line 4933, in _apply_standard\n",
      "    labels=labels)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"pandas/_libs/src/reduce.pyx\", line 622, in pandas._libs.lib.reduce\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"pandas/_libs/src/reduce.pyx\", line 134, in pandas._libs.lib.Reducer.get_result\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 568, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-125-be59fa339bb0>\", line 34, in findMatches\n",
      "    companyWordList = comp_record['comp_vec']\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/series.py\", line 623, in __getitem__\n",
      "    result = self.index.get_value(self, key)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2554, in get_value\n",
      "    s = _values_from_object(series)\n",
      "  File \"pandas/_libs/lib.pyx\", line 85, in pandas._libs.lib.values_from_object\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/series.py\", line 413, in get_values\n",
      "    return self._data.get_values()\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\", line 4515, in get_values\n",
      "    return np.array(self._block.to_dense(), copy=False)\n",
      "  File \"/usr/local/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\", line 179, in to_dense\n",
      "    return self.values.view()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "## Custom Approach\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "tick_df = ticker.copy();\n",
    "head_df = headlines.copy();\n",
    "head_df['matches'] = None\n",
    "head_df.astype({'matches':'object'})\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "# 1. Vectorize companies and headlines in their stripped form.\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "def vectorize(s):\n",
    "    tokens = s.lower().translate(translator).split()\n",
    "    return [ np.array(list(word)) for word in tokens ]\n",
    "tick_df['comp_vec'] = tick_df['company'].apply(vectorize)\n",
    "head_df['head_vec'] = head_df['headline_text'].apply(vectorize)\n",
    "\n",
    "def percentage_matching(W, C):\n",
    "    if len(W) < len(C):\n",
    "        return np.sum(W == C[0:len(W)]) / len(C)\n",
    "    else:\n",
    "        return np.sum(W[0:len(C)] == C) / len(W)\n",
    "\n",
    "# 2. Headline opertion\n",
    "MIN_SCORE = 0.3\n",
    "def findMatches(head_record):\n",
    "    matchedCompanies = []\n",
    "    headWordList = head_record['head_vec']\n",
    "    \n",
    "    for _,comp_record in tick_df.iterrows():\n",
    "        \n",
    "        companyWordList = comp_record['comp_vec']\n",
    "        matchingHeads = []\n",
    "        for wIdx in range(len(headWordList)):\n",
    "            if companyWordList[0][0] != headWordList[wIdx][0]:\n",
    "                continue\n",
    "                          \n",
    "            companyMatchScore = 0\n",
    "            for cIdx,C in enumerate(companyWordList):\n",
    "                if  wIdx+cIdx < len(headWordList):\n",
    "                    weight = 1/(cIdx+1)\n",
    "                    W = headWordList[wIdx+cIdx]\n",
    "                    matches = percentage_matching(W, C)\n",
    "                    if matches == 1.0:\n",
    "                        companyMatchScore += (matches * weight)\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            normalizedScore = companyMatchScore / len(companyWordList) # Normalizes score out of 100%.\n",
    "            if normalizedScore >= MIN_SCORE:\n",
    "                matchedCompanies.append((comp_record['company'], normalizedScore))\n",
    "                \n",
    "    matchedCompanies.sort(key=lambda x: x[1], reverse=True)\n",
    "    return matchedCompanies[:10]\n",
    "\n",
    "# Uncomment for parallel and comment the bottom part\n",
    "# tqdm.pandas(desc=\"HALP\")\n",
    "# def appfun(df):\n",
    "#     print('GO')\n",
    "#     return df.progress_apply(findMatches, axis=1)\n",
    "\n",
    "# print(\"Looping...\")\n",
    "# parallelize(head_df, appfun)\n",
    "\n",
    "tqdm_notebook().pandas(\"matching\")\n",
    "head_df['matches'] = head_df.progress_apply(findMatches, axis=1)\n",
    "head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Approach\n",
    "\n",
    "\"\"\"\n",
    "First we want to clean up the company column to make computing percentage of matching \n",
    "characters easier:\n",
    "\n",
    "1. lowercase the entire company column\n",
    "   a. remove punctuation (except spaces).\n",
    "   b. Tokenize on spaces and produce a list of each word vectorized.\n",
    "   \n",
    "Then we want to look at each headline, first cleaning it so it can match the same as\n",
    "the companies would (no punctuation).\n",
    "   \n",
    "2. for each headline:\n",
    "    a. remove punctuation (except spaces).\n",
    "    b. tokenize on spaces and produce a list of each work vectorized.\n",
    "    \n",
    "    \n",
    "    With each cleaned headline, we now have a list of words inside it. For each company we\n",
    "    also have a list of cleaned words. So now we want to try to score each company on how\n",
    "    well its words matches the headline words.\n",
    "    \n",
    "    c. for each company:\n",
    "        i. matchedCompanies = []\n",
    "        \n",
    "        \n",
    "        We do this by under the notion the first word of a company MUST be present. So we\n",
    "        find each word in the headline that starts with the company's first letter. From there\n",
    "        we compare each word starting from this word in the headline to each word in the\n",
    "        company. We do a percentage matching.\n",
    "        \n",
    "        We also want to weigh each successive word less and less, and we want to have a scoring\n",
    "        system where higher is better. This allows headlines like \"quantas under fire for...\"\n",
    "        to score well with companies like \"quantas airlines limited\". If Quantas is found, it should\n",
    "        score super high, yet not allow the mismatch between \"under\" and \"airlines\" to drag it down.\n",
    "        In the event two companies start with the same first word, then the second word matching would\n",
    "        help distinguish.\n",
    "        \n",
    "        \n",
    "        ii. For each word (W) in headline:\n",
    "            1. If it doesn't start with the company's first word first letter, skip\n",
    "            2. CompanyMatchScore = 0\n",
    "            2. for i,C in Company Word List: (where C is the ith word in their name list)\n",
    "            3.   if W+i exists\n",
    "            4.     CompanyMatchScore += percentage_matching(W+i, C) * Weight (the first word should weight far more than latter words)\n",
    "            \n",
    "            \n",
    "            We want to normalize this CompanyMatchScore to be out of 100% when done, as we'll later need\n",
    "            to compare how different company names of different lengths performed. If we don't normalize\n",
    "            then names with more words will score higher than those with less.\n",
    "            \n",
    "            \n",
    "            5. normalizedScore = companyMatchScore / len(CompanyWordList) # Normalizes score out of 100%.\n",
    "            6. matchCompanies.append((company, normalizedScore))\n",
    "            \n",
    "        \n",
    "        Before moving tot the next company, we'll sort the matches to make later filtering easier. We'll\n",
    "        then append these matches into a new column associated with the headlines ('matches'). We can limit\n",
    "        the number of matches to keep to some top N (10? 5?) to prevent filling memory with huge vectors.\n",
    "        \n",
    "        \n",
    "        iii. Sort matchedCompanies by their normalizedScore\n",
    "        iV. df.loc[headlineIndex, 'matches'] = matchedCompanies[:10].\n",
    "\n",
    "\n",
    "Now we have all our matches with their headlines, we can inspect how well it did and start keeping or discarding matches.\n",
    "The thought is that some headlines won't have any companies associated so their match score should be low. We'll drop\n",
    "matches that don't meet a threshold and retain the highest one above the threshold (for those that do meet the threshold).\n",
    "This will leave some false matches behind (for example if \"Bell Labs\" is a company and a headline has \"time to ring the bell\"\n",
    "in it). But once we match enough companies, we can probably keep only those companies who had more than N headlines matched \n",
    "to them for further analysis, allowing us to account for these false positives:\n",
    "        \n",
    "\n",
    "3. Ideally, the correct answer is the first element in each match. \n",
    "    a. If the first match is > some percentage threshold, keep it as the match\n",
    "    b. else the headline had no matches and use NA. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
